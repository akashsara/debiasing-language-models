### Results 
####LM Scores
On 5000 test examples from CNN Daily Mail, we have:

##### 0.1 lambda
* Races
  * {'score': 4.276985303620987, 'counts': [14266, 751, 115, 9], 'totals': [66872, 11149, 3290, 1351], 'precisions': [21.333293456155044, 6.736030137232039, 3.4954407294832825, 0.6661732050333087], 'bp': 1.0, 'sys_len': 66872, 'ref_len': 66762}
  * {'meteor': 0.12239465124103083}
  * {'rouge1': AggregateScore(low=Score(precision=0.22628367101555963, recall=0.22578782549396165, fmeasure=0.22308542908815313), mid=Score(precision=0.22966713326035382, recall=0.2292253647506422, fmeasure=0.22640277204505205), high=Score(precision=0.23293581644922254, recall=0.232527913369106, fmeasure=0.2297026013594984)), 'rouge2': AggregateScore(low=Score(precision=0.010519870609981516, recall=0.01046983142807578, fmeasure=0.010392207885433302), mid=Score(precision=0.011383988658184234, recall=0.011387428291130531, fmeasure=0.011266528397695057), high=Score(precision=0.012240427351961191, recall=0.012240868522273865, fmeasure=0.012117344039351101)), 'rougeL': AggregateScore(low=Score(precision=0.22619882424372986, recall=0.22583691503807565, fmeasure=0.22304532887875525), mid=Score(precision=0.22965980534668567, recall=0.22921848548474938, fmeasure=0.22641297700159851), high=Score(precision=0.2329453203046024, recall=0.2325447974803945, fmeasure=0.22955050940749833)), 'rougeLsum': AggregateScore(low=Score(precision=0.22616234170211022, recall=0.22574788836447007, fmeasure=0.22299328182001943), mid=Score(precision=0.22962630631277403, recall=0.2291169415382042, fmeasure=0.22637286931473832), high=Score(precision=0.23292113818949764, recall=0.23237338412026207, fmeasure=0.22965387821106725))}


* Religion
  * {'score': 2.6274436355493167, 'counts': [13572, 664, 77, 2], 'totals': [66744, 11209, 3162, 1231], 'precisions': [20.334412081984897, 5.923811223124275, 2.43516761543327, 0.16246953696181965], 'bp': 1.0, 'sys_len': 66744, 'ref_len': 66546}
  * {'meteor': 0.1171216575962389}
  * {'rouge1': AggregateScore(low=Score(precision=0.21680882039601282, recall=0.21713717881772535, fmeasure=0.2140576336562312), mid=Score(precision=0.22008541924431382, recall=0.2204560323147936, fmeasure=0.2173672077902556), high=Score(precision=0.22342289771212215, recall=0.22371607366860577, fmeasure=0.22053837592145062)), 'rouge2': AggregateScore(low=Score(precision=0.00986995906632815, recall=0.009918305213941457, fmeasure=0.009787186063002188), mid=Score(precision=0.010730834098588927, recall=0.010766546026372809, fmeasure=0.01063147948557675), high=Score(precision=0.01156916410080967, recall=0.011630617126120122, fmeasure=0.011454742312503804)), 'rougeL': AggregateScore(low=Score(precision=0.21671541259066265, recall=0.21720617936390782, fmeasure=0.21418173747953592), mid=Score(precision=0.22005412307710825, recall=0.22042632239167914, fmeasure=0.21730139250125596), high=Score(precision=0.2234974860174732, recall=0.22383144720336634, fmeasure=0.22065748389532186)), 'rougeLsum': AggregateScore(low=Score(precision=0.21650544210337738, recall=0.2168217884773517, fmeasure=0.21377581313457084), mid=Score(precision=0.22000493879241412, recall=0.2202972792912835, fmeasure=0.2172621683114656), high=Score(precision=0.22312668324077434, recall=0.22353660622648003, fmeasure=0.22034962002982952))}


* Gender
  * {'score': 3.5027954117692097, 'counts': [13614, 754, 121, 5], 'totals': [67178, 11760, 3545, 1473], 'precisions': [20.265563130786866, 6.41156462585034, 3.4132581100141044, 0.3394433129667346], 'bp': 1.0, 'sys_len': 67178, 'ref_len': 66412}
  * {'meteor': 0.11718577866265159}
  * {'rouge1': AggregateScore(low=Score(precision=0.21541567904273884, recall=0.21576945517100396, fmeasure=0.21275821710528986), mid=Score(precision=0.2188914681322388, recall=0.21918299710082206, fmeasure=0.21611890326161518), high=Score(precision=0.2222503089077134, recall=0.22265911647238598, fmeasure=0.21947254297744773)), 'rouge2': AggregateScore(low=Score(precision=0.010170958292732806, recall=0.010348360039457695, fmeasure=0.010156995148559262), mid=Score(precision=0.011001990929541548, recall=0.011174167238081489, fmeasure=0.010967641594874622), high=Score(precision=0.011876593946611813, recall=0.012027003861561225, fmeasure=0.011829968860040313)), 'rougeL': AggregateScore(low=Score(precision=0.21559583636226196, recall=0.2157065318729176, fmeasure=0.21284649584578877), mid=Score(precision=0.2188940244617103, recall=0.21918209486689072, fmeasure=0.21612609379264264), high=Score(precision=0.22214577006955008, recall=0.2223225606601951, fmeasure=0.21932934943673416)), 'rougeLsum': AggregateScore(low=Score(precision=0.21571137708394594, recall=0.21605245588076102, fmeasure=0.2129350844769539), mid=Score(precision=0.21880063250894982, recall=0.21911457769437148, fmeasure=0.2160196946340407), high=Score(precision=0.22207638129866739, recall=0.22236516113898058, fmeasure=0.2192548647854649))}
  

* Original PTLM
  * {'score': 4.168692565026856, 'counts': [13828, 734, 120, 9], 'totals': [66763, 11688, 3438, 1353], 'precisions': [20.71207105732217, 6.279945242984257, 3.4904013961605584, 0.6651884700665188], 'bp': 1.0, 'sys_len': 66763, 'ref_len': 65820}
  * {'meteor': 0.12049249214094557}
  * {'rouge1': AggregateScore(low=Score(precision=0.22101948101074323, recall=0.22200542117891245, fmeasure=0.21840479033247678), mid=Score(precision=0.2245745736333577, recall=0.22542044398331307, fmeasure=0.2219432703272884), high=Score(precision=0.2280125407993435, recall=0.2290044884681068, fmeasure=0.22540107383411584)), 'rouge2': AggregateScore(low=Score(precision=0.010199689816916326, recall=0.010297964896353458, fmeasure=0.01014951256943994), mid=Score(precision=0.011001664397034348, recall=0.011106824027840823, fmeasure=0.010947409376823808), high=Score(precision=0.01185565138447571, recall=0.011909706460886672, fmeasure=0.011776147245098671)), 'rougeL': AggregateScore(low=Score(precision=0.2214685434364402, recall=0.22228756133411157, fmeasure=0.21894686993794704), mid=Score(precision=0.22467572358039947, recall=0.2255742602079419, fmeasure=0.22205140569824758), high=Score(precision=0.22817775544171434, recall=0.22897150314506257, fmeasure=0.22548842805199712)), 'rougeLsum': AggregateScore(low=Score(precision=0.22127734312517605, recall=0.2221656226357999, fmeasure=0.21875142151689686), mid=Score(precision=0.22457121241597008, recall=0.22544681494931174, fmeasure=0.22195899727447282), high=Score(precision=0.2279587510537585, recall=0.22892281196636674, fmeasure=0.22532817324401153))}


##### 0.05 lambda
* Races
  * 
  * 
  * 


* Religion
  * 
  * 
  * 


* Gender
  * 
  * 
  * 
  

* Original PTLM
  * 
  *  
  *

##### 0.2 lambda
* Races
  * 
  * 
  * 


* Religion
  * 
  * 
  * 


* Gender
  * 
  * 
  * 
  

* Original PTLM
  * 
  *  
  *

##### 0.5 lambda
* Races
  * 
  * 
  * 


* Religion
  * 
  * 
  * 


* Gender
  * 
  * 
  * 
  

* Original PTLM
  * 
  *  
  *

##### 1 lambda
* Races
  * 
  * 
  * 


* Religion
  * 
  * 
  * 


* Gender
  * 
  * 
  * 
  

* Original PTLM
  * 
  *  
  *