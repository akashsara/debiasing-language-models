### Results 
####LM Scores
On 5000 test examples from CNN Daily Mail, we have:

##### No regularisation 

* PTLM no training: 
  * {'score': 0.151455115266975, 'counts': [14454, 689, 29, 0], 'totals': [135997, 81151, 57369, 43345], 'precisions': [10.628175621521063, 0.8490345159024535, 0.050549948578500585, 0.0011535355865728459], 'bp': 1.0, 'sys_len': 135997, 'ref_len': 65638}
  * {'meteor': 0.12230863622731737}
  * {'rouge1': AggregateScore(low=Score(precision=0.20324693536739036, recall=0.2346932299548442, fmeasure=0.20793137344627569), mid=Score(precision=0.20639278674288833, recall=0.237966221184994, fmeasure=0.21109682546068026), high=Score(precision=0.20966287743453188, recall=0.24142572367981294, fmeasure=0.21420582091469892)), 'rouge2': AggregateScore(low=Score(precision=0.010685267772960734, recall=0.011729660084720522, fmeasure=0.010842759229152305), mid=Score(precision=0.011536287530268021, recall=0.012677235461070495, fmeasure=0.011712181062383947), high=Score(precision=0.012439645013261868, recall=0.013602863420059438, fmeasure=0.012601166428886002)), 'rougeL': AggregateScore(low=Score(precision=0.20333250603570405, recall=0.23485168590199465, fmeasure=0.2080921949608649), mid=Score(precision=0.20657671553405155, recall=0.23816237290401837, fmeasure=0.21127006383407237), high=Score(precision=0.20964764146077472, recall=0.24167055627472825, fmeasure=0.2144070019156368)), 'rougeLsum': AggregateScore(low=Score(precision=0.20306091931778303, recall=0.23451595650932042, fmeasure=0.2077743100021213), mid=Score(precision=0.20651408841657773, recall=0.23811208148728943, fmeasure=0.2111885780830483), high=Score(precision=0.20964895377565998, recall=0.2412118028333367, fmeasure=0.21424010850473507))}

* Original PTLM trained with our data
  * {'score': 4.168692565026856, 'counts': [13828, 734, 120, 9], 'totals': [66763, 11688, 3438, 1353], 'precisions': [20.71207105732217, 6.279945242984257, 3.4904013961605584, 0.6651884700665188], 'bp': 1.0, 'sys_len': 66763, 'ref_len': 65820}
  * {'meteor': 0.12049249214094557}
  * {'rouge1': AggregateScore(low=Score(precision=0.22101948101074323, recall=0.22200542117891245, fmeasure=0.21840479033247678), mid=Score(precision=0.2245745736333577, recall=0.22542044398331307, fmeasure=0.2219432703272884), high=Score(precision=0.2280125407993435, recall=0.2290044884681068, fmeasure=0.22540107383411584)), 'rouge2': AggregateScore(low=Score(precision=0.010199689816916326, recall=0.010297964896353458, fmeasure=0.01014951256943994), mid=Score(precision=0.011001664397034348, recall=0.011106824027840823, fmeasure=0.010947409376823808), high=Score(precision=0.01185565138447571, recall=0.011909706460886672, fmeasure=0.011776147245098671)), 'rougeL': AggregateScore(low=Score(precision=0.2214685434364402, recall=0.22228756133411157, fmeasure=0.21894686993794704), mid=Score(precision=0.22467572358039947, recall=0.2255742602079419, fmeasure=0.22205140569824758), high=Score(precision=0.22817775544171434, recall=0.22897150314506257, fmeasure=0.22548842805199712)), 'rougeLsum': AggregateScore(low=Score(precision=0.22127734312517605, recall=0.2221656226357999, fmeasure=0.21875142151689686), mid=Score(precision=0.22457121241597008, recall=0.22544681494931174, fmeasure=0.22195899727447282), high=Score(precision=0.2279587510537585, recall=0.22892281196636674, fmeasure=0.22532817324401153))}


##### 0.1 lambda
* Races
  * {'score': 4.276985303620987, 'counts': [14266, 751, 115, 9], 'totals': [66872, 11149, 3290, 1351], 'precisions': [21.333293456155044, 6.736030137232039, 3.4954407294832825, 0.6661732050333087], 'bp': 1.0, 'sys_len': 66872, 'ref_len': 66762}
  * {'meteor': 0.12239465124103083}
  * {'rouge1': AggregateScore(low=Score(precision=0.22628367101555963, recall=0.22578782549396165, fmeasure=0.22308542908815313), mid=Score(precision=0.22966713326035382, recall=0.2292253647506422, fmeasure=0.22640277204505205), high=Score(precision=0.23293581644922254, recall=0.232527913369106, fmeasure=0.2297026013594984)), 'rouge2': AggregateScore(low=Score(precision=0.010519870609981516, recall=0.01046983142807578, fmeasure=0.010392207885433302), mid=Score(precision=0.011383988658184234, recall=0.011387428291130531, fmeasure=0.011266528397695057), high=Score(precision=0.012240427351961191, recall=0.012240868522273865, fmeasure=0.012117344039351101)), 'rougeL': AggregateScore(low=Score(precision=0.22619882424372986, recall=0.22583691503807565, fmeasure=0.22304532887875525), mid=Score(precision=0.22965980534668567, recall=0.22921848548474938, fmeasure=0.22641297700159851), high=Score(precision=0.2329453203046024, recall=0.2325447974803945, fmeasure=0.22955050940749833)), 'rougeLsum': AggregateScore(low=Score(precision=0.22616234170211022, recall=0.22574788836447007, fmeasure=0.22299328182001943), mid=Score(precision=0.22962630631277403, recall=0.2291169415382042, fmeasure=0.22637286931473832), high=Score(precision=0.23292113818949764, recall=0.23237338412026207, fmeasure=0.22965387821106725))}


* Religion
  * {'score': 2.6274436355493167, 'counts': [13572, 664, 77, 2], 'totals': [66744, 11209, 3162, 1231], 'precisions': [20.334412081984897, 5.923811223124275, 2.43516761543327, 0.16246953696181965], 'bp': 1.0, 'sys_len': 66744, 'ref_len': 66546}
  * {'meteor': 0.1171216575962389}
  * {'rouge1': AggregateScore(low=Score(precision=0.21680882039601282, recall=0.21713717881772535, fmeasure=0.2140576336562312), mid=Score(precision=0.22008541924431382, recall=0.2204560323147936, fmeasure=0.2173672077902556), high=Score(precision=0.22342289771212215, recall=0.22371607366860577, fmeasure=0.22053837592145062)), 'rouge2': AggregateScore(low=Score(precision=0.00986995906632815, recall=0.009918305213941457, fmeasure=0.009787186063002188), mid=Score(precision=0.010730834098588927, recall=0.010766546026372809, fmeasure=0.01063147948557675), high=Score(precision=0.01156916410080967, recall=0.011630617126120122, fmeasure=0.011454742312503804)), 'rougeL': AggregateScore(low=Score(precision=0.21671541259066265, recall=0.21720617936390782, fmeasure=0.21418173747953592), mid=Score(precision=0.22005412307710825, recall=0.22042632239167914, fmeasure=0.21730139250125596), high=Score(precision=0.2234974860174732, recall=0.22383144720336634, fmeasure=0.22065748389532186)), 'rougeLsum': AggregateScore(low=Score(precision=0.21650544210337738, recall=0.2168217884773517, fmeasure=0.21377581313457084), mid=Score(precision=0.22000493879241412, recall=0.2202972792912835, fmeasure=0.2172621683114656), high=Score(precision=0.22312668324077434, recall=0.22353660622648003, fmeasure=0.22034962002982952))}


* Gender
  * {'score': 3.5027954117692097, 'counts': [13614, 754, 121, 5], 'totals': [67178, 11760, 3545, 1473], 'precisions': [20.265563130786866, 6.41156462585034, 3.4132581100141044, 0.3394433129667346], 'bp': 1.0, 'sys_len': 67178, 'ref_len': 66412}
  * {'meteor': 0.11718577866265159}
  * {'rouge1': AggregateScore(low=Score(precision=0.21541567904273884, recall=0.21576945517100396, fmeasure=0.21275821710528986), mid=Score(precision=0.2188914681322388, recall=0.21918299710082206, fmeasure=0.21611890326161518), high=Score(precision=0.2222503089077134, recall=0.22265911647238598, fmeasure=0.21947254297744773)), 'rouge2': AggregateScore(low=Score(precision=0.010170958292732806, recall=0.010348360039457695, fmeasure=0.010156995148559262), mid=Score(precision=0.011001990929541548, recall=0.011174167238081489, fmeasure=0.010967641594874622), high=Score(precision=0.011876593946611813, recall=0.012027003861561225, fmeasure=0.011829968860040313)), 'rougeL': AggregateScore(low=Score(precision=0.21559583636226196, recall=0.2157065318729176, fmeasure=0.21284649584578877), mid=Score(precision=0.2188940244617103, recall=0.21918209486689072, fmeasure=0.21612609379264264), high=Score(precision=0.22214577006955008, recall=0.2223225606601951, fmeasure=0.21932934943673416)), 'rougeLsum': AggregateScore(low=Score(precision=0.21571137708394594, recall=0.21605245588076102, fmeasure=0.2129350844769539), mid=Score(precision=0.21880063250894982, recall=0.21911457769437148, fmeasure=0.2160196946340407), high=Score(precision=0.22207638129866739, recall=0.22236516113898058, fmeasure=0.2192548647854649))}
  

##### 0.01 lambda
* Religion
  * {'score': 4.36798132327046, 'counts': [13781, 691, 122, 15], 'totals': [66604, 11898, 3713, 1627], 'precisions': [20.69094949252297, 5.8076987729030085, 3.2857527605709667, 0.9219422249539029], 'bp': 1.0, 'sys_len': 66604, 'ref_len': 65388} 
  * {'meteor': 0.12047526927769374} 
  * {'rouge1': AggregateScore(low=Score(precision=0.22144073550184773, recall=0.22350313183565826, fmeasure=0.21937221854882888), mid=Score(precision=0.22493515732018152, recall=0.22695467716763013, fmeasure=0.22275483481955805), high=Score(precision=0.22813392111500708, recall=0.23021495246973753, fmeasure=0.22593230533386346)), 'rouge2': AggregateScore(low=Score(precision=0.009309168235631026, recall=0.009380160064830218, fmeasure=0.009197338723084978), mid=Score(precision=0.010081129166895155, recall=0.010159425058341102, fmeasure=0.00997356480018663), high=Score(precision=0.010844590271811652, recall=0.01094318368764509, fmeasure=0.010733918441707208)), 'rougeL': AggregateScore(low=Score(precision=0.2217286507821773, recall=0.22377841435804824, fmeasure=0.21966936369219991), mid=Score(precision=0.2249998961409298, recall=0.2269375295404875, fmeasure=0.22278781934447217), high=Score(precision=0.22850703762911084, recall=0.23054131772986336, fmeasure=0.2263509829987341)), 'rougeLsum': AggregateScore(low=Score(precision=0.22138914700340834, recall=0.22360436159036012, fmeasure=0.21930722649963036), mid=Score(precision=0.22505981062541502, recall=0.2270458120184851, fmeasure=0.22282644416511704), high=Score(precision=0.22880140232693422, recall=0.23075285155464595, fmeasure=0.22655399916993066))}

##### 0.2 lambda
* Religion
  * {'score': 3.127563707114711, 'counts': [13481, 617, 92, 4], 'totals': [66449, 11091, 3301, 1315], 'precisions': [20.287739469367484, 5.563069155170859, 2.787034232050894, 0.3041825095057034], 'bp': 1.0, 'sys_len': 66449, 'ref_len': 66181}
  * {'meteor': 0.1170900488866317}
  * {'rouge1': AggregateScore(low=Score(precision=0.2167979334513534, recall=0.2168230846748181, fmeasure=0.21390754106016274), mid=Score(precision=0.22031590048498217, recall=0.22043671581859414, fmeasure=0.2174999395252077), high=Score(precision=0.22348676500779696, recall=0.2235842090186997, fmeasure=0.2205243894040421)), 'rouge2': AggregateScore(low=Score(precision=0.008743271071931789, recall=0.008757767621662632, fmeasure=0.008613855871478986), mid=Score(precision=0.009507960306851162, recall=0.009486734829052108, fmeasure=0.009362091597721504), high=Score(precision=0.010281124679359804, recall=0.010269593675108686, fmeasure=0.010140118260534462)), 'rougeL': AggregateScore(low=Score(precision=0.21672135076618212, recall=0.21676758875991625, fmeasure=0.21395417943870465), mid=Score(precision=0.22021202395719236, recall=0.2203193412919201, fmeasure=0.21737930209695475), high=Score(precision=0.223724838024014, recall=0.2239734761096174, fmeasure=0.2209453704385746)), 'rougeLsum': AggregateScore(low=Score(precision=0.21687429857716967, recall=0.21704712314131955, fmeasure=0.2141205108156782), mid=Score(precision=0.2202884170395646, recall=0.22036368469133424, fmeasure=0.2174522459007051), high=Score(precision=0.22384669348488992, recall=0.22387233574017817, fmeasure=0.2209563949664716))}

##### 0.5 lambda

* Religion
  * {'score': 2.232140417583886, 'counts': [13078, 505, 52, 1], 'totals': [66058, 9967, 2529, 786], 'precisions': [19.797753489357838, 5.066720176582723, 2.0561486753657574, 0.1272264631043257], 'bp': 0.9862290964013747, 'sys_len': 66058, 'ref_len': 66974}
  * {'meteor': 0.1135802654442815}
  * {'rouge1': AggregateScore(low=Score(precision=0.21126517517028068, recall=0.20983080109617827, fmeasure=0.20790802318680793), mid=Score(precision=0.21454547499768695, recall=0.21333617310645753, fmeasure=0.2113333046744792), high=Score(precision=0.2177612347197713, recall=0.21639399326434727, fmeasure=0.2144299552278745)), 'rouge2': AggregateScore(low=Score(precision=0.007628934826145608, recall=0.007551567987734217, fmeasure=0.007512989096799337), mid=Score(precision=0.008351755777813432, recall=0.0082589007149097, fmeasure=0.00822770141377405), high=Score(precision=0.009052402940459848, recall=0.008969241946123262, fmeasure=0.00892447307139504)), 'rougeL': AggregateScore(low=Score(precision=0.21141846561412606, recall=0.21005540741193565, fmeasure=0.2081058241190856), mid=Score(precision=0.21440221290063544, recall=0.21315460166345385, fmeasure=0.21119190192971798), high=Score(precision=0.21783353007994719, recall=0.2167516762726561, fmeasure=0.21460274954404662)), 'rougeLsum': AggregateScore(low=Score(precision=0.21118490913150528, recall=0.20985255464971497, fmeasure=0.20798987961172785), mid=Score(precision=0.21440717931999986, recall=0.21325089501668687, fmeasure=0.21123702418428525), high=Score(precision=0.21786828970949454, recall=0.2165209139315284, fmeasure=0.2145038647708158))}

##### 1 lambda

* Religion
  * {'score': 2.5182234955446003, 'counts': [12629, 556, 98, 2], 'totals': [66295, 10330, 3321, 1462], 'precisions': [19.049702089146994, 5.382381413359148, 2.9509183980728695, 0.13679890560875513], 'bp': 0.9928157342616362, 'sys_len': 66295, 'ref_len': 66773}
  * {'meteor': 0.10834182604233966}
  * {'rouge1': AggregateScore(low=Score(precision=0.20268319325854198, recall=0.20139253381152342, fmeasure=0.19943222556898282), mid=Score(precision=0.20587380292955226, recall=0.2044446358906291, fmeasure=0.20253464821408854), high=Score(precision=0.2090249938311787, recall=0.20756659561886082, fmeasure=0.2057042762696047)), 'rouge2': AggregateScore(low=Score(precision=0.006909794812233835, recall=0.006984797045772655, fmeasure=0.006861122810600164), mid=Score(precision=0.007602668334375652, recall=0.007679650972333898, fmeasure=0.0075468936792978634), high=Score(precision=0.00830286786384347, recall=0.008387757229220643, fmeasure=0.008259079867093806)), 'rougeL': AggregateScore(low=Score(precision=0.20273105267748154, recall=0.20138810715030248, fmeasure=0.19940538162314958), mid=Score(precision=0.20582279315728835, recall=0.20440458109447684, fmeasure=0.20250614400439965), high=Score(precision=0.20939795865187882, recall=0.20807883115722872, fmeasure=0.2059497407676828)), 'rougeLsum': AggregateScore(low=Score(precision=0.20259624744206656, recall=0.20141333337587713, fmeasure=0.1994950043181729), mid=Score(precision=0.20584413940685747, recall=0.20442887348810723, fmeasure=0.20249085667203934), high=Score(precision=0.20867559018604342, recall=0.20733662620770663, fmeasure=0.20536568880209735))}
