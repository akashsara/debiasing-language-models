
Lmod is automatically replacing "intel/2020.1.217" with "gcc/9.3.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) libfabric/1.10.1     2) openmpi/4.0.3     3) ucx/1.8.0

created virtual environment CPython3.10.2.final.0-64 in 2875ms
  creator CPython3Posix(dest=/scratch/deep1401/debiasing_language_models/ENV, clear=False, no_vcs_ignore=False, global=False)
  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/deep1401/.local/share/virtualenv)
    added seed packages: Pillow_SIMD==9.0.0.post1+computecanada, PyYAML==6.0+computecanada, Pygments==2.14.0, aiohttp==3.8.1+computecanada, aiosignal==1.3.1, async_timeout==4.0.2+computecanada, colorama==0.4.6, datasets==2.8.0, dill==0.3.6+computecanada, filelock==3.9.0, frozenlist==1.3.0+computecanada, fsspec==2023.1.0, huggingface_hub==0.11.1+computecanada, joblib==1.2.0+computecanada, markdown_it_py==2.1.0+computecanada, mdurl==0.1.2, multidict==6.0.4+computecanada, multiprocess==0.70.14, pip==22.3.1, python_dateutil==2.8.2+computecanada, regex==2022.1.18+computecanada, responses==0.18.0+computecanada, rich==13.2.0, scikit_learn==1.1.2+computecanada, setuptools==66.1.1, six==1.16.0+computecanada, threadpoolctl==3.1.0+computecanada, tokenizers==0.12.1+computecanada, torch==1.13.1+computecanada, torchvision==0.14.1+computecanada, tqdm==4.64.1+computecanada, transformers==4.25.1, wheel==0.38.4+computecanada, xxhash==3.0.0+computecanada, yarl==1.7.2+computecanada
  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
Traceback (most recent call last):
  File "/scratch/deep1401/debiasing_language_models/ENV/bin/pip", line 5, in <module>
    from pip._internal.cli.main import main
ModuleNotFoundError: No module named 'pip._internal'
Traceback (most recent call last):
  File "/scratch/deep1401/debiasing_language_models/ENV/bin/pip", line 5, in <module>
    from pip._internal.cli.main import main
ModuleNotFoundError: No module named 'pip._internal'
race
2000
0.02
Bias Type: race
Debias Size: 2000
LM CNN Fraction: 0.02
{'OUTPUT_PATH': '../models/race/new_debsize_2000/lm_0.02/', 'DOWNSTREAM_OUTPUT_PATH': '../models/downstream/race/new_debsize_2000/lm_0.02/', 'MODEL': 'bert-base-uncased', 'TRAIN_EPOCHS': 30, 'VAL_EPOCHS': 1, 'LEARNING_RATE': 0.0001, 'MAX_SOURCE_TEXT_LENGTH': 64, 'MAX_TARGET_TEXT_LENGTH': 32, 'EARLY_STOPPING_PATIENCE': 3, 'BATCH_SIZE': 64, 'WORD_LIST': '../word_lists/race.csv'}
{'OUTPUT_PATH': '../models/race/new_debsize_2000/lm_0.02/', 'DOWNSTREAM_OUTPUT_PATH': '../models/downstream/race/new_debsize_2000/lm_0.02/', 'MODEL': 'bert-base-uncased', 'LM_TRAIN_EPOCHS': 50, 'LM_VAL_EPOCHS': 1, 'LEARNING_RATE': 0.0001, 'MAX_SOURCE_TEXT_LENGTH': 64, 'MAX_TARGET_TEXT_LENGTH': 32, 'EARLY_STOPPING_PATIENCE': 3, 'BATCH_SIZE': 64, 'WORD_LIST': '../word_lists/race.csv', 'DOWNLOAD_CNN_DATA': True, 'CNN_DATA_PATH': '../data/cnn_dailmail_saved.pkl', 'MLM_PROBABILITY': 0.15}
Debias Model: True

LM Fine tune Model: True

INTERLEAVING EPOCH: 0
Starting Model debiasing...
[12:21:25] [Model]: Loading bert-base-uncased...                  trainer.py:180
                                                                                
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[12:22:00] [Initiating Fine Tuning]...                            trainer.py:198
                                                                                
           [Epoch: 1/30]                                          trainer.py:200
Processing batches..:   0%|          | 0/32 [00:00<?, ?it/s]Processing batches..:   0%|          | 0/32 [00:03<?, ?it/s]
Traceback (most recent call last):
  File "/scratch/deep1401/debiasing_language_models/src/main.py", line 118, in <module>
    debiasing_trainer.train_model(
  File "/scratch/deep1401/debiasing_language_models/src/trainer.py", line 202, in train_model
    train_losses = self.train(model, training_loader, optimizer)
  File "/scratch/deep1401/debiasing_language_models/src/trainer.py", line 77, in train
    sentence1_y_hat = model(
  File "/scratch/deep1401/debiasing_language_models/ENV/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/deep1401/debiasing_language_models/ENV/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 1021, in forward
    encoder_outputs = self.encoder(
  File "/scratch/deep1401/debiasing_language_models/ENV/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/deep1401/debiasing_language_models/ENV/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 610, in forward
    layer_outputs = layer_module(
  File "/scratch/deep1401/debiasing_language_models/ENV/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/deep1401/debiasing_language_models/ENV/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 496, in forward
    self_attention_outputs = self.attention(
  File "/scratch/deep1401/debiasing_language_models/ENV/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/deep1401/debiasing_language_models/ENV/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 426, in forward
    self_outputs = self.self(
  File "/scratch/deep1401/debiasing_language_models/ENV/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/deep1401/debiasing_language_models/ENV/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 285, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/scratch/deep1401/debiasing_language_models/ENV/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/deep1401/debiasing_language_models/ENV/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
